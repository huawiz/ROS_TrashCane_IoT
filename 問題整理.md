# 問題整理

### 模型選擇與架構
1. 影像辨識的部分，使用的是什麼模型?

```
在前期，我們使用Google提供的建議機器學習辨識服務Teachable Machine
作為我們的辨識模型，然而因為在測試階段發現
這個模型對於帶有數字的回收標章比較難辨識，
後續就改採用監督式學習yolov5作為我們的辨識模型，
採用這模型的主要原因是因為教學資源容易取得，使用起來也算簡易，適合快速開發。
```

2. 你們是在哪裡做模型訓練的?

```
在前期學習摸索階段，我們是使用自己的筆記型電腦進行訓練，
後來發現受硬體限制，單次執行訓練時間過長(約1小時)，
所以後續的訓練改成付費訂閱Google的Colab進行訓練，
使用雲端的GPU，可以使訓練模型的時間縮短至10分鐘，
這也使我們在開發效能上提升不少。
```

---

### 訓練資料與準備
3. 在辨識的部分使用多少資料作為訓練集?是如何做出訓練資料的?

```
我們的辨識功能需辨識4種回收物，8個圖示。
我們的訓練集是沿用Teachable Machine做測試時，留下網路攝影機的逐偵記錄圖，
另外還有收集各類別約10張不同場景的圖做為訓練資料。
使用第三方工具進行資料預處理-製作圖片標籤資料。
整體訓練資料，約一個圖示150張圖。
訓練的過程有針對模型標錯的圖，重新處理後加入訓練集。
```

4. 訓練的資料集是否有考量光線等等的影響?

```
因為我們的辨識設計是對固定裝置進行辨識，
認為光線對辨識效果的影響較小，
所以沒有特別針對這塊做處理。
```

5. 辨識實際物品時，標章在商品上的標示通常不明顯，你們有考慮過這個問題嗎?

```
因為我們設定此次專題目標，
主要聚焦在驗證整體回收服務的可行性，
包含從辨識、回收物處理自動化、使用者回饋體驗的部分，
也因為人手關係，在準備大量訓練資料的部分能力有限，
所以在這部分沒有多加著墨，但是若這個商業模式經過驗證確認可行，
辨識回收物的部分我認為可以透過更多人協作處理的方式豐富訓練集，
或是與產業合作，推出更容易辨別的商品包裝，使其可行性提升。
```

---

### 系統整合與通訊
6. 從鏡頭辨識到回收物處理的部分是怎麼做溝通的?

```
簡易流程是鏡頭傳回影像資料到可以執行辨識模型的python環境進行辨識，
辨識完之後再透過wifi連線，使用http的方式與控制開關的esp32做溝通，
執行開左或開右的工作。
```

7. 為何採用http做溝通?

```
此次專題使用http作為主要溝通方式的主要原因在於容易實作，
ESP32與Python都有現成的HTTP函式庫可以使用，
而辨識到回收物處理的ESP32的發送命令的動作也相對單純。

但通訊方式確實仍有改善空間，若未來有需要更即時命令傳送的需求，
針對通訊這段的程式調整即可。
```

---

### 效能與可靠性
8. 你是怎麼處理辨識與開關之間的敏感度的，會不會有模型一瞬間辨識錯，導致箱子開關開錯的情況?

```
有在辨識模型與控制開關操作的程式部分，
設計可調整的參數，分別是偵測的辨識次數、辨識結果的時間間隔數，
只要在設定秒數內辨識超過特定次數，才會產生辨識結果的指令給esp32。
```

9. 回收系統需要連續運作，如何確保即時辨識效果？當大量物品同時投入時，如何維持辨識效率？

```
目前系統的即時性與大量回收物處理的部分確實還有改善空間，
未來可在回收物進入辨識處這段設置更長的緩衝區，
或是多設計一些量化回收物的機制，使大量處理回收物的部分變得可行，
目前我們的系統只有辦法做單一回收物的辨識與處理。
```

### 擴展性與未來規劃
10. 如何應對未來可能增加的新回收物種類？擴充性如何？

```
辨識模型的部分，訓練模型是依照辨識類別進行檔案管理，
若未來需要新增類別，只需要再準備新類別的訓練資料，並且重新訓練即可。

程式的部分也有將辨識與傳送指令的部分模組化，
若有新的功能或需要套用新的模型，可以再針對這塊額外切割與擴充，原有程式只需做微調。
```